{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BL6m8iiXJgZC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkk9IUVU8GC-",
        "outputId": "190ee19b-98d1-4847-fb30-0ec1fb8fb9bf"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# PART 1 ‚Äî Setup + Load Model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# PART 1 ‚Äî Setup + Load Model\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from google.colab import files\n",
        "\n",
        "# Load CLIP model (cached)\n",
        "print(\"üöÄ Loading CLIP model...\")\n",
        "model = SentenceTransformer('clip-ViT-B-32', cache_folder=\"/content/clip_cache\")\n",
        "\n",
        "# Dataset folder & embedding file\n",
        "dataset_folder = \"/content/drive/MyDrive/Datasets/polyvore-outfit-dataset/polyvore_outfits/images\"\n",
        "embedding_file = \"/content/fashion_embeddings.pkl\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# üì¶ Polyvore Outfit Dataset Setup (Full Cell)\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1Ô∏è‚É£ Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2Ô∏è‚É£ Define paths\n",
        "zip_path = \"/content/drive/MyDrive/polyvore-outfit-dataset.zip\"   # <-- your zip file\n",
        "extract_dir = \"/content/drive/MyDrive/Datasets/polyvore-outfit-dataset\"\n",
        "images_dir = os.path.join(extract_dir, \"polyvore_outfits\", \"images\")\n",
        "\n",
        "# 3Ô∏è‚É£ Check if already extracted\n",
        "if os.path.exists(images_dir):\n",
        "    print(\"‚úÖ Dataset already extracted at:\", images_dir)\n",
        "else:\n",
        "    # Create folder if not exists\n",
        "    os.makedirs(\"/content/drive/MyDrive/Datasets\", exist_ok=True)\n",
        "\n",
        "    # 4Ô∏è‚É£ Extract from ZIP\n",
        "    print(\"üì¶ Extracting dataset... Please wait (may take a few minutes)...\")\n",
        "    !unzip -q \"{zip_path}\" -d \"/content/drive/MyDrive/Datasets/\"\n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# 5Ô∏è‚É£ Verify images folder\n",
        "if os.path.exists(images_dir):\n",
        "    print(\"‚úÖ Images found at:\", images_dir)\n",
        "    print(\"üì∏ Sample files:\", len(os.listdir(images_dir)), \"images found.\")\n",
        "else:\n",
        "    print(\"‚ùå Images folder not found! Please check zip structure.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "Ey8bQfE79uyW",
        "outputId": "43d82378-1afd-4fed-a4ef-ba21d9bbb22f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öôÔ∏è Computing image embeddings... (one-time process)\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Datasets/polyvore-outfit-dataset/polyvore_outfits/images'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1441400792.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚öôÔ∏è Computing image embeddings... (one-time process)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mimage_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Datasets/polyvore-outfit-dataset/polyvore_outfits/images'"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# PART 2 ‚Äî Compute and Save Embeddings (Run Once)\n",
        "# ============================================\n",
        "\n",
        "dataset_folder = \"/content/drive/MyDrive/Datasets/polyvore-outfit-dataset/polyvore_outfits/images\"\n",
        "\n",
        "if not os.path.exists(embedding_file):\n",
        "    print(\"‚öôÔ∏è Computing image embeddings... (one-time process)\")\n",
        "    images = [os.path.join(dataset_folder, img) for img in os.listdir(dataset_folder)[:1000]]\n",
        "    image_embeddings = []\n",
        "    for img_path in tqdm(images):\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            emb = model.encode(img, convert_to_tensor=True, show_progress_bar=False)\n",
        "            image_embeddings.append((img_path, emb))\n",
        "        except:\n",
        "            pass\n",
        "    with open(embedding_file, \"wb\") as f:\n",
        "        pickle.dump(image_embeddings, f)\n",
        "    print(\"‚úÖ Embeddings saved successfully!\")\n",
        "else:\n",
        "    print(\"‚úÖ Embeddings already exist. Skip this part next time.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH5Ioy6895Tf"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# üß† Step 2: Create or Load Embeddings\n",
        "# ============================================\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pickle\n",
        "\n",
        "# Model (you can also use \"clip-ViT-B-32\" for speed)\n",
        "model = SentenceTransformer(\"clip-ViT-B-32\")\n",
        "\n",
        "# Dataset path (same as before)\n",
        "dataset_folder = \"/content/drive/MyDrive/Datasets/polyvore-outfit-dataset/polyvore_outfits/images\"\n",
        "\n",
        "# File to save embeddings\n",
        "embed_file = \"/content/drive/MyDrive/Datasets/polyvore_embeddings.pkl\"\n",
        "\n",
        "# If embeddings already exist, load them\n",
        "if os.path.exists(embed_file):\n",
        "    print(\"‚úÖ Loading precomputed embeddings...\")\n",
        "    with open(embed_file, \"rb\") as f:\n",
        "        image_embeddings = pickle.load(f)\n",
        "else:\n",
        "    print(\"‚öôÔ∏è Creating embeddings (this may take 10‚Äì15 mins for all images)...\")\n",
        "    image_embeddings = []\n",
        "    image_files = [os.path.join(dataset_folder, f) for f in os.listdir(dataset_folder) if f.endswith(\".jpg\")]\n",
        "\n",
        "    for img_path in tqdm(image_files[:3000]):  # limit to first 3k to stay under 3GB\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            emb = model.encode(img, convert_to_tensor=True)\n",
        "            image_embeddings.append((img_path, emb))\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error:\", img_path, e)\n",
        "\n",
        "    # Save embeddings\n",
        "    with open(embed_file, \"wb\") as f:\n",
        "        pickle.dump(image_embeddings, f)\n",
        "    print(f\"‚úÖ Saved embeddings to {embed_file}\")\n",
        "\n",
        "print(f\"üì¶ Total embedded images: {len(image_embeddings)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp3qh9V3-8Sn"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# üëó Step 3: Smart Outfit Recommender (Enhanced)\n",
        "# ============================================\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pickle\n",
        "from google.colab import files\n",
        "from IPython.display import display, Image as IPyImage\n",
        "\n",
        "# Load model + embeddings\n",
        "model = SentenceTransformer(\"clip-ViT-B-32\")\n",
        "embed_file = \"/content/drive/MyDrive/Datasets/polyvore_embeddings.pkl\"\n",
        "\n",
        "with open(embed_file, \"rb\") as f:\n",
        "    image_embeddings = pickle.load(f)\n",
        "\n",
        "# -----------------------------------------\n",
        "# üß† Upload image from local system\n",
        "# -----------------------------------------\n",
        "print(\"üì§ Please upload your photo...\")\n",
        "uploaded = files.upload()   # opens file picker\n",
        "user_image_path = list(uploaded.keys())[0]\n",
        "print(\"‚úÖ Uploaded:\", user_image_path)\n",
        "\n",
        "# -----------------------------------------\n",
        "# üß† Text + Gender input\n",
        "# -----------------------------------------\n",
        "user_text = input(\"üìù Describe your occasion (e.g., I am going to a party): \")\n",
        "gender = input(\"üë§ Enter gender (Male/Female): \").strip().lower()\n",
        "\n",
        "# -----------------------------------------\n",
        "# üîç Encode User Inputs\n",
        "# -----------------------------------------\n",
        "user_img = Image.open(user_image_path).convert(\"RGB\")\n",
        "user_img_emb = model.encode(user_img, convert_to_tensor=True)\n",
        "text_emb = model.encode(user_text, convert_to_tensor=True)\n",
        "\n",
        "# -----------------------------------------\n",
        "# üí¨ Suitability Analysis\n",
        "# -----------------------------------------\n",
        "fit_score = util.cos_sim(user_img_emb, text_emb).item()\n",
        "\n",
        "print(\"\\nüß† Suitability Analysis:\")\n",
        "if fit_score > 0.45:\n",
        "    print(f\"‚úÖ Your outfit seems suitable for this occasion! (score: {fit_score:.2f})\")\n",
        "else:\n",
        "    print(f\"‚ùå Your outfit may not fit well for this occasion. (score: {fit_score:.2f})\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# üí° Recommendation Logic (with gender filter)\n",
        "# -----------------------------------------\n",
        "scores = []\n",
        "for img_path, emb in image_embeddings:\n",
        "    # Optional gender-based filter (if filenames contain hints)\n",
        "    if gender == \"male\" and \"women\" in img_path.lower():\n",
        "        continue\n",
        "    if gender == \"female\" and \"men\" in img_path.lower():\n",
        "        continue\n",
        "\n",
        "    sim = 0.6 * util.cos_sim(text_emb, emb).item() + 0.4 * util.cos_sim(user_img_emb, emb).item()\n",
        "    scores.append((img_path, sim))\n",
        "\n",
        "# Top 5 results\n",
        "top_results = sorted(scores, key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "# -----------------------------------------\n",
        "# üñºÔ∏è Display Recommendations\n",
        "# -----------------------------------------\n",
        "print(\"\\n‚ú® Recommended Outfits üëï\\n\")\n",
        "for path, score in top_results:\n",
        "    print(f\"Score: {score:.4f} | {os.path.basename(path)}\")\n",
        "    display(IPyImage(filename=path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDzHZ6ylGiLd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
